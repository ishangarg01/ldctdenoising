# configs/default_config.yaml

# --- Model Configuration (CGNet - now based on CascadedGaze architecture) ---
model:
  # The 'arch' key is required by the define_CGNet function in CGNet_arch.py
  arch:
    type: CGNet # Corresponds to the class name in models/archs/CGNet_arch.py
    args:
      img_channel: 3       # Assuming input images are converted to 3 channels (RGB)
      out_channels: 3      # Assuming output images are 3 channels (RGB)
      # Reduced width further to help with CUDA OOM. Original was 16, trying 12.
      width: 16            # Reduced base channel width for CGNet to save memory
      # Reduced number of blocks slightly to also help with memory.
      middle_blk_num: 8    # Number of blocks in the middle part of CGNet (reduced from 10)
      enc_blk_nums: [2, 2, 3, 4] # Number of blocks in each encoder stage (reduced from [2, 2, 4, 6])
      dec_blk_nums: [2, 2, 2, 2] # Number of blocks in each decoder stage (keeping same for now)
      GCE_CONVS_nums: [3, 3, 2, 2] # Number of GCE convolutions in CascadedGazeBlock for each encoder stage (keeping same)
      # Add any other architecture-specific arguments here that your CGNet class constructor expects

  # Other model-related settings that are NOT architecture arguments
  # can go directly under the 'model' key, but outside the 'arch' section.
  # For example:
  # pretrained: true
  # freeze_encoder: false

# --- Autoencoder Configuration (for Perceptual Loss) ---
autoencoder:
  # Path to the pre-trained autoencoder checkpoint
  # This checkpoint should be generated by training the AE separately (scripts/train_ae.py)
  model_path: data/processed/pretrained_autoencoder.pth # Example path to save/load AE checkpoint
  # Autoencoder architecture details - used for loading the correct AE model class
  arch:
    type: SimpleConvAE # Corresponds to the class name in models/autoencoder/simple_conv_ae.py
    args:
      in_channels: 3      # AE input channels (matches processed data)
      base_channels: 16   # Modest base channel count for AE for small GPU
      num_encoder_layers: 3 # Number of downsampling steps in the AE encoder
      # The AE decoder will mirror the encoder structure

# --- Dataset Configuration ---
dataset:
  type: CTDenoiseDataset # Corresponds to the class name in datasets/ct_denoise_dataset.py
  args:
    root: data/processed/split # Base directory containing 'train' and 'test' subfolders
    # mode will be set dynamically in train.py/test.py
    transform: # Use Compose to chain multiple transforms if needed
      type: Compose
      args:
        transforms:
          # Removed Resize transform - using original image size
          - type: ToTensor # Convert PIL Image to PyTorch Tensor
            args: {}
  # Keeping batch size 1 to minimize memory per iteration
  train_batch_size: 2
  test_batch_size: 1 # Smaller batch size for testing/evaluation is common
  num_workers: 4 # Number of subprocesses for data loading. Adjust based on your CPU cores.

# --- Loss Configuration ---
loss:
  type: CombinedLoss # Corresponds to a class name in losses/combined_loss.py
  args:
    pixel_loss_type: mse # Options: 'mse', 'mae'. MAE is often more robust to outliers.
    pixel_loss_weight: 100.0 # Weight for the pixel loss component (start at 1.0)
    ssim_loss_weight: 20.0 # Increased weight for SSIM
    feature_loss_weight: 200.0 # Adjusted weight for perceptual feature loss as requested
    # Removed gradient_loss_weight line
    learnable_weights: false # Set to true to use learnable loss weights (requires implementing LossWeights module)

# --- Optimizer Configuration ---
optimizer:
  type: Adam # Options: 'Adam', 'AdamW', 'SGD', etc.
  args:
    lr: 0.0001
    weight_decay: 0.0

# --- Learning Rate Scheduler Configuration (Optional) ---
# scheduler:
#   type: StepLR # Options: 'StepLR', 'CosineAnnealingLR', etc.
#   args:
#     step_size: 50 # Example: decay learning rate every 50 epochs
#     gamma: 0.1    # Example: decay rate by a factor of 0.1

# --- Training Configuration ---
training:
  epochs: 50 #changed for now to 5#100
  device: cuda # Use 'cuda' if GPU is available, 'cpu' otherwise. Script should handle fallback.
  seed: 42 # Random seed for reproducibility
  save_interval: 10 # Save a model checkpoint every N epochs
  log_interval: 50 # Log training progress every N batches (iterations)
  eval_interval: 5 # Evaluate on test set every N epochs (optional, set to -1 to disable)
  experiment_name: cgnet_denoising_run # Name for this experiment run
  output_dir: experiments/ # Base directory to save experiment results
  resume_checkpoint: null # Path to a checkpoint to resume training from (set to null to start fresh)

# --- Evaluation Configuration ---
evaluation:
  metrics: ["psnr", "ssim"] # Metrics to calculate during evaluation (for CGNet)
  # Add any other evaluation-specific settings here

# --- Autoencoder Training Configuration (for scripts/train_ae.py) ---
# This section is used by train_ae.py, not train.py
ae_training:
  epochs: 50 # Fewer epochs needed for AE training typically
  device: cuda
  seed: 42
  save_interval: 10
  log_interval: 20 # Log AE training progress more frequently
  experiment_name: simple_conv_ae_train
  output_dir: data/processed/ # Save AE checkpoint here
  loss_type: mse # Reconstruction loss for AE: 'mse' or 'mae'
  optimizer:
    type: Adam
    args:
      lr: 0.001 # Higher learning rate for AE training might be okay
      weight_decay: 0.0

# --- Autoencoder Evaluation Configuration (for scripts/evaluate_ae.py) ---
# This section is used by evaluate_ae.py
ae_evaluation:
  metrics: ["psnr", "ssim"] # Metrics to calculate for AE reconstruction
  # The AE model_path is taken from the main autoencoder config section above
