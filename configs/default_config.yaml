# configs/default_config.yaml

# --- Model Configuration (CGNet - now based on CascadedGaze architecture) ---
model:
  # The 'arch' key is required by the define_CGNet function in CGNet_arch.py
  arch:
    type: CGNet # Corresponds to the class name in models/archs/CGNet_arch.py
    args:
      img_channel: 3       # Assuming input images are converted to 3 channels (RGB)
      out_channels: 3      # Assuming output images are 3 channels (RGB)
      # Reduced width further to help with CUDA OOM. Original was 16, trying 12.
      width: 16            # Reduced base channel width for CGNet to save memory
      # Reduced number of blocks slightly to also help with memory.
      middle_blk_num: 8    # Number of blocks in the middle part of CGNet (reduced from 10)
      enc_blk_nums: [2, 2, 3, 4] # Number of blocks in each encoder stage (reduced from [2, 2, 4, 6])
      dec_blk_nums: [2, 2, 2, 2] # Number of blocks in each decoder stage (keeping same for now)
      GCE_CONVS_nums: [3, 3, 2, 2] # Number of GCE convolutions in CascadedGazeBlock for each encoder stage (keeping same)
      # Add any other architecture-specific arguments here that your CGNet class constructor expects

  # Other model-related settings that are NOT architecture arguments
  # can go directly under the 'model' key, but outside the 'arch' section.
  # For example:
  # pretrained: true
  # freeze_encoder: false

# # --- Autoencoder Configuration (for Perceptual Loss) ---
# autoencoder:
#   # Path to the pre-trained autoencoder checkpoint
#   # This checkpoint should be generated by training the AE separately (scripts/train_ae.py)
#   model_path: data/processed/pretrained_autoencoder.pth # Example path to save/load AE checkpoint
#   # Autoencoder architecture details - used for loading the correct AE model class
#   arch:
#     type: SimpleConvAE # Corresponds to the class name in models/autoencoder/simple_conv_ae.py
#     args:
#       in_channels: 3      # AE input channels (matches processed data)
#       base_channels: 16   # Modest base channel count for AE for small GPU
#       num_encoder_layers: 3 # Number of downsampling steps in the AE encoder
#       # The AE decoder will mirror the encoder structure

# configs/default_config.yaml

# ... other config ...

# --- Autoencoder Configuration (for Perceptual Loss) ---
autoencoder:
  # Path to the pre-trained autoencoder checkpoint
  model_path: data/processed/pretrained_autoencoder.pth # Keep this path, it's where the trained AE will be saved
  # Autoencoder architecture details - used for loading the correct AE model class
  arch:
    type: EnhancedConvAE # <-- Changed to the new class name
    args:
      in_channels: 3
      base_channels: 32   # <-- Updated base channels (should match ae_config.yaml)
      num_encoder_layers: 4 # <-- Updated number of layers (should match ae_config.yaml)

# ... rest of default_config.yaml ...


# --- Dataset Configuration ---
dataset:
  type: CTDenoiseDataset # Corresponds to the class name in datasets/ct_denoise_dataset.py
  args:
    root: data/split # Base directory containing 'train' and 'test' subfolders
    # mode will be set dynamically in train.py/test.py
    transform: # Use Compose to chain multiple transforms if needed
      type: Compose
      args:
        transforms:
          # Removed Resize transform - using original image size
          - type: ToTensor # Convert PIL Image to PyTorch Tensor
            args: {}
  # Keeping batch size 1 to minimize memory per iteration
  train_batch_size: 2
  test_batch_size: 1 # Smaller batch size for testing/evaluation is common
  num_workers: 4 # Number of subprocesses for data loading. Adjust based on your CPU cores.

# --- Loss Configuration ---
loss:
  type: CombinedLoss # Corresponds to a class name in losses/combined_loss.py
  args:
    pixel_loss_type: mae # Options: 'mse', 'mae'. MAE is often more robust to outliers.
    # Fixed weights are ignored when learnable_weights is true, but can be kept as defaults.
    # If you want to use fixed weights for some and learnable for others,
    # you'd need a more complex LossWeights implementation.
    # Assuming pixel, ssim, feature, and gradient weights are LEARNED if learnable_weights is true.
    # If gradient loss is disabled (_has_gradient_loss_module is False), only pixel, ssim, feature are learned.
    # pixel_loss_weight: 1.0 # Ignored if learnable_weights is true
    # ssim_loss_weight: 1.0 # Ignored if learnable_weights is true
    # feature_loss_weight: 1.0 # Ignored if learnable_weights is true
    # gradient_loss_weight: 1.0 # Ignored if learnable_weights is true (if gradient loss enabled)

    learnable_weights: true # Set to true to enable learnable weights
    # Define initial values for the learnable lambdas.
    # Order should match LossWeights: [pixel, ssim, feature, (gradient)].
    # Adjust these based on the typical magnitudes of your raw losses and desired emphasis.
    initial_learnable_weights: [30.0, 2.0, 20.0, 5.0] # Suggested initial weights (pixel, ssim, feature, gradient)
    # If gradient loss is NOT enabled in CombinedLoss, this list should only have 3 values: [pixel, ssim, feature]
    # initial_learnable_weights: [15.0, 10.0, 50.0] # Example initial weights if gradient loss is disabled


# --- Optimizer Configuration ---
optimizer:
  type: Adam # Options: 'Adam', 'AdamW', 'SGD', etc.
  args:
    lr: 0.0001
    weight_decay: 0.0

# --- Learning Rate Scheduler Configuration (Optional) ---
# scheduler:
#   type: StepLR # Options: 'StepLR', 'CosineAnnealingLR', etc.
#   args:
#     step_size: 50 # Example: decay learning rate every 50 epochs
#     gamma: 0.1    # Example: decay rate by a factor of 0.1

# --- Training Configuration ---
training:
  epochs: 50 #changed for now to 5#100
  device: cuda # Use 'cuda' if GPU is available, 'cpu' otherwise. Script should handle fallback.
  seed: 42 # Random seed for reproducibility
  save_interval: 10 # Save a model checkpoint every N epochs
  log_interval: 50 # Log training progress every N batches (iterations)
  eval_interval: 5 # Evaluate on test set every N epochs (optional, set to -1 to disable)
  experiment_name: cgnet_denoising_run # Name for this experiment run
  output_dir: experiments/ # Base directory to save experiment results
  resume_checkpoint: null # Path to a checkpoint to resume training from (set to null to start fresh)

# --- Evaluation Configuration ---
evaluation:
  metrics: ["psnr", "ssim"] # Metrics to calculate during evaluation (for CGNet)
  # Add any other evaluation-specific settings here

# --- Autoencoder Training Configuration (for scripts/train_ae.py) ---
# This section is used by train_ae.py, not train.py
ae_training:
  epochs: 50 # Fewer epochs needed for AE training typically
  device: cuda
  seed: 42
  save_interval: 10
  log_interval: 20 # Log AE training progress more frequently
  experiment_name: simple_conv_ae_train
  output_dir: data/processed/ # Save AE checkpoint here
  loss_type: mse # Reconstruction loss for AE: 'mse' or 'mae'
  optimizer:
    type: Adam
    args:
      lr: 0.001 # Higher learning rate for AE training might be okay
      weight_decay: 0.0

# --- Autoencoder Evaluation Configuration (for scripts/evaluate_ae.py) ---
# This section is used by evaluate_ae.py
ae_evaluation:
  metrics: ["psnr", "ssim"] # Metrics to calculate for AE reconstruction
  # The AE model_path is taken from the main autoencoder config section above
